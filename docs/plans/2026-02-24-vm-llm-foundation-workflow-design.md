# VM Web App - LLM in Chat + Foundation Workflow (Design)

Date: 2026-02-24  
Status: Approved  
Branch: `main`

## Context

The v2 async workflow is already running in production shape (`requested_mode -> effective_mode`, fallback to `foundation_stack`, gates, timeline, artifacts).  
The app also already supports LLM client bootstrapping through `KIMI_API_KEY`, but the workflow execution path still relies on Foundation state progression and template artifacts.

Goal for this cycle:
1. Enable real LLM behavior in `/api/chat` (operational validation).
2. Evolve Foundation workflow generation so all Foundation stages produce LLM-based outputs.

## Scope Decisions (Approved)

1. Use `.env` for API key setup (project-local).
2. Deliver both:
   - chat with real LLM now;
   - implementation path to run LLM on all Foundation stages now.
3. Apply integration in all Foundation stages (`research`, `brand-voice`, `positioning`, `keywords`) plus final brief.

## Architecture

### Runtime boundaries

- Keep `WorkflowRuntimeV2` as async coordinator and source of state transitions/events.
- Keep `executor.py` as canonical state/gate driver (`run_until_gate`, `approve_stage`).
- Extend `FoundationRunnerService` to generate stage artifacts with LLM content, stage by stage.

### LLM lifecycle

- `Settings` reads:
  - `KIMI_API_KEY`
  - `KIMI_MODEL` (default `kimi-for-coding`)
  - `KIMI_BASE_URL` (default `https://api.kimi.com/coding/v1`)
- `create_app` initializes `KimiClient` when key is present.
- `FoundationRunnerService` receives/injects `llm` and calls `llm.chat(...)` for stage content.

### Failure model

- On LLM call failure for a stage:
  - capture structured metadata (`error_code`, `error_message`, `retryable`);
  - stage fails in run semantics;
  - previously completed stage artifacts/history remain intact.

## Data Flow per Foundation Stage

For each stage:
1. Resolve stage input:
   - `request_text`
   - run/thread context
   - previous stage artifacts (when applicable)
2. Build stage-specific prompt.
3. Execute `llm.chat(...)`.
4. Persist:
   - markdown artifact(s) for stage
   - `output.json` metadata (model, stage key, status, optional usage fields)
   - run/stage manifests already used by v2 runtime

Final step (`foundation-brief`) is also generated by LLM from cumulative stage outputs.

## Security and Config

### `.env` usage

- Store API key in project `.env`.
- `.env` is already git-ignored and must not be committed.

### Logging safety

- Never log full API key.
- Log only operational metadata (run/stage IDs, status, duration, error code/message).

## Testing and Acceptance

### Unit

1. `FoundationRunnerService` with `FakeLLM`:
   - validates prompt routing per stage;
   - validates artifact generation for all Foundation stages + final brief.
2. Error path:
   - LLM failure maps to structured stage failure output.

### Integration

1. Run started in non-foundation mode still mapped to `foundation_stack`.
2. After approvals, all stage artifacts include LLM-produced content.
3. `/api/chat` returns real LLM output when key is configured.

### Regression targets

- `test_vm_webapp_api_v2.py`
- `test_vm_webapp_event_driven_e2e.py`
- `test_vm_webapp_workflow_runtime_v2.py`
- `test_vm_webapp_foundation_runner_service.py`
- `test_vm_webapp_ui_assets.py`

### Done criteria

1. `.env` with `KIMI_API_KEY` activates app LLM mode.
2. `/api/chat` returns non-placeholder model output.
3. Foundation workflow artifacts (`research`, `brand-voice`, `positioning`, `keywords`, brief) are LLM-generated.
4. Regression suite remains green.

## Risks and Mitigations

1. Risk: provider latency/timeouts during stage generation.  
Mitigation: strict timeout handling and structured failure metadata.

2. Risk: prompt drift between stages causing inconsistent outputs.  
Mitigation: stage-specific prompt contracts and tests with deterministic fake LLM.

3. Risk: mixing state progression and content generation concerns.  
Mitigation: keep state progression in executor, content generation in `FoundationRunnerService`.

## Next Step

After this approved design, produce a task-by-task implementation plan via writing-plans skill.
