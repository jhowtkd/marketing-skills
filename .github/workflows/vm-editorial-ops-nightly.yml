name: vm-editorial-ops-nightly

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: "0 6 * * *"
  workflow_dispatch:
    inputs:
      threads:
        description: "Comma-separated list of thread IDs to include (default: all)"
        required: false
        default: ""
      staging_url:
        description: "Staging URL for testing (if empty, tests will be SKIPPED)"
        required: false
        default: ""

env:
  STAGING_URL: ${{ inputs.staging_url || vars.STAGING_URL || '' }}

jobs:
  collect-and-report:
    runs-on: ubuntu-latest
    outputs:
      has_staging_url: ${{ steps.check-staging.outputs.has_staging_url }}
      report_status: ${{ steps.generate-report.outputs.report_status }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - uses: astral-sh/setup-uv@v4

      - name: Check staging URL
        id: check-staging
        run: |
          if [ -n "$STAGING_URL" ]; then
            echo "has_staging_url=true" >> $GITHUB_OUTPUT
            echo "âœ… Staging URL configured"
          else
            echo "has_staging_url=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ No staging URL - tests will run in SKIPPED mode"
          fi

      - name: Install dependencies
        run: uv sync --frozen

      - name: Collect editorial insights and forecasts
        id: collect
        run: |
          # Create logs directory
          mkdir -p ./logs
          mkdir -p ./traces
          
          # Start the webapp in background for API access
          uv run python -c "
          import subprocess
          import time
          import json
          import sys
          
          # Start app
          proc = subprocess.Popen(
              ['uv', 'run', 'python', '-m', 'vm_webapp.app'],
              cwd='09-tools',
              env={'VM_DB_PATH': '/tmp/test_workspace.sqlite3', 'VM_WORKSPACE_ROOT': '/tmp/test_vm'},
              stdout=open('./logs/webapp.log', 'w'),
              stderr=subprocess.STDOUT,
          )
          time.sleep(3)  # Wait for startup
          
          try:
              import urllib.request
              
              # Get all threads (you may need to adjust this endpoint)
              # For now, we'll query a sample set or use input
              thread_ids = '${{ github.event.inputs.threads }}'.split(',') if '${{ github.event.inputs.threads }}' else []
              
              insights = []
              forecasts = []
              recommendations = []
              for thread_id in thread_ids:
                  if not thread_id.strip():
                      continue
                  thread_id = thread_id.strip()
                  try:
                      # Fetch insights
                      url = f'http://localhost:8000/api/v2/threads/{thread_id}/editorial-decisions/insights'
                      req = urllib.request.Request(url)
                      with urllib.request.urlopen(req, timeout=10) as resp:
                          data = json.loads(resp.read().decode())
                          insights.append(data)
                      
                      # Fetch forecast
                      forecast_url = f'http://localhost:8000/api/v2/threads/{thread_id}/editorial-decisions/forecast'
                      forecast_req = urllib.request.Request(forecast_url)
                      with urllib.request.urlopen(forecast_req, timeout=10) as resp:
                          forecast_data = json.loads(resp.read().decode())
                          forecasts.append(forecast_data)
                      
                      # Fetch recommendations
                      recs_url = f'http://localhost:8000/api/v2/threads/{thread_id}/editorial-decisions/recommendations'
                      recs_req = urllib.request.Request(recs_url)
                      with urllib.request.urlopen(recs_req, timeout=10) as resp:
                          recs_data = json.loads(resp.read().decode())
                          recommendations.append(recs_data)
                  except Exception as e:
                      print(f'Warning: Failed to fetch {thread_id}: {e}', file=sys.stderr)
              
              # If no specific threads, create sample data for demo
              if not insights:
                  insights = [{
                      'thread_id': 'sample',
                      'totals': {'marked_total': 0, 'by_scope': {'global': 0, 'objective': 0}, 'by_reason_code': {}},
                      'policy': {'denied_total': 0},
                      'baseline': {'resolved_total': 0, 'by_source': {'objective_golden': 0, 'global_golden': 0, 'previous': 0, 'none': 0}},
                      'recency': {'last_marked_at': None, 'last_actor_id': None}
                  }]
                  forecasts = [{
                      'thread_id': 'sample',
                      'risk_score': 45,
                      'trend': 'stable',
                      'drivers': ['no_golden_marks'],
                      'recommended_focus': 'Iniciar marcaÃ§Ãµes golden'
                  }]
              
              with open('/tmp/insights.json', 'w') as f:
                  json.dump(insights, f)
              
              with open('/tmp/forecasts.json', 'w') as f:
                  json.dump(forecasts, f)
              
              with open('/tmp/recommendations.json', 'w') as f:
                  json.dump(recommendations, f)
              
              print(f'Collected {len(insights)} thread insights, {len(forecasts)} forecasts, {len(recommendations)} recommendation sets')
          finally:
              proc.terminate()
              proc.wait()
          "
          
          # Copy the files for the next step
          cp /tmp/insights.json ./insights.json
          cp /tmp/forecasts.json ./forecasts.json
          cp /tmp/recommendations.json ./recommendations.json
          
          # Create trace file
          cat > ./traces/collection-trace.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "staging_url": "$STAGING_URL",
            "has_staging_url": "$([ -n "$STAGING_URL" ] && echo "true" || echo "false")",
            "insights_file": "./insights.json",
            "forecasts_file": "./forecasts.json",
            "recommendations_file": "./recommendations.json"
          }
          EOF

      - name: Download previous report for delta
        uses: actions/download-artifact@v4
        with:
          name: editorial-ops-report
          path: ./previous-report
        continue-on-error: true  # May not exist on first run

      - name: Generate report with forecast delta
        id: generate-report
        run: |
          PREVIOUS_REPORT=""
          if [ -f "./previous-report/editorial-ops-report.md" ]; then
            PREVIOUS_REPORT="--previous-report ./previous-report/editorial-ops-report.md"
          fi
          
          # Set staging URL parameter
          STAGING_PARAM=""
          if [ -n "$STAGING_URL" ]; then
            STAGING_PARAM="--staging-url $STAGING_URL"
          fi
          
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file ./insights.json \
            --forecasts-file ./forecasts.json \
            --recommendations-file ./recommendations.json \
            $PREVIOUS_REPORT \
            $STAGING_PARAM \
            --output ./editorial-ops-report.md \
            --github-step-summary \
            2>&1 | tee ./logs/report-generation.log
          
          # Determine status
          if grep -q "SKIPPED" ./editorial-ops-report.md; then
            echo "report_status=skipped" >> $GITHUB_OUTPUT
          else
            echo "report_status=success" >> $GITHUB_OUTPUT
          fi

      - name: Generate evidence summary
        run: |
          # Create evidence summary file
          cat > ./evidence-summary.md << 'EOF'
          ## ðŸ“Š EvidÃªncia AutomÃ¡tica - Nightly E2E
          
          ### Resumo da ExecuÃ§Ã£o
          
          | Item | Valor |
          |------|-------|
          | Timestamp | $(date -u +%Y-%m-%dT%H:%M:%SZ) |
          | Staging URL | ${{ env.STAGING_URL || 'NÃ£o configurado' }} |
          | Status | ${{ steps.generate-report.outputs.report_status == 'skipped' && 'âš ï¸ SKIPPED' || 'âœ… SUCCESS' }} |
          
          ### Top 3 Riscos Identificados
          
          EOF
          
          # Extract top 3 risks from report
          if grep -q "## Top 3 Threads by Risk" ./editorial-ops-report.md; then
            awk '/## Top 3 Threads by Risk/{found=1} found{print} /^## [^#]/{if(found && $0 !~ "Top 3") exit}' ./editorial-ops-report.md >> ./evidence-summary.md
          else
            echo "_Nenhum risco identificado no relatÃ³rio._" >> ./evidence-summary.md
          fi
          
          # Add AÃ§Ãµes section
          cat >> ./evidence-summary.md << 'EOF'
          
          ### AÃ§Ãµes Executadas/Pendentes
          
          EOF
          
          if grep -q "## AÃ§Ãµes Executadas e Pendentes" ./editorial-ops-report.md; then
            awk '/## AÃ§Ãµes Executadas e Pendentes/{found=1} found{print} /^## [^#]/{if(found && $0 !~ "AÃ§Ãµes") exit}' ./editorial-ops-report.md >> ./evidence-summary.md
          else
            echo "_Nenhuma aÃ§Ã£o registrada._" >> ./evidence-summary.md
          fi
          
          # Add SLO alerts section
          cat >> ./evidence-summary.md << 'EOF'
          
          ### SLO Alerts Evidence
          
          EOF
          
          if grep -q "## SLO Alerts Evidence" ./editorial-ops-report.md; then
            awk '/## SLO Alerts Evidence/{found=1} found{print} /^## [^#]/{if(found && $0 !~ "SLO") exit}' ./editorial-ops-report.md >> ./evidence-summary.md
          else
            echo "_Nenhum alerta SLO registrado._" >> ./evidence-summary.md
          fi
          
          # Add to job summary
          cat ./evidence-summary.md >> $GITHUB_STEP_SUMMARY

      - name: Upload logs artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: logs
          path: ./logs/
          retention-days: 7

      - name: Upload traces artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: traces
          path: ./traces/
          retention-days: 7

      - name: Upload evidence artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: evidence
          path: |
            ./evidence-summary.md
            ./editorial-ops-report.md
            ./insights.json
            ./forecasts.json
            ./recommendations.json
          retention-days: 30
          overwrite: true

      - name: Upload report artifact
        uses: actions/upload-artifact@v4
        with:
          name: editorial-ops-report
          path: |
            ./editorial-ops-report.md
            ./insights.json
            ./forecasts.json
            ./recommendations.json
          retention-days: 30
          overwrite: true

      - name: Publish summary
        run: |
          echo "## Editorial Operations Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Generated at: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Include report content
          cat ./editorial-ops-report.md >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“„ Full report available as artifact: \"editorial-ops-report\"" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š Evidence summary available as artifact: \"evidence\"" >> $GITHUB_STEP_SUMMARY

  test-report-generator:
    runs-on: ubuntu-latest
    needs: collect-and-report
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Test report generator with sample data
        run: |
          # Create sample insights
          echo '[
            {
              "thread_id": "t-1",
              "totals": {"marked_total": 5, "by_scope": {"global": 2, "objective": 3}, "by_reason_code": {"clarity": 2, "cta": 3}},
              "policy": {"denied_total": 1},
              "baseline": {"resolved_total": 8, "by_source": {"objective_golden": 3, "global_golden": 2, "previous": 2, "none": 1}},
              "recency": {"last_marked_at": "2026-02-27T10:00:00Z", "last_actor_id": "user-1"}
            },
            {
              "thread_id": "t-2",
              "totals": {"marked_total": 3, "by_scope": {"global": 1, "objective": 2}, "by_reason_code": {"structure": 3}},
              "policy": {"denied_total": 0},
              "baseline": {"resolved_total": 5, "by_source": {"objective_golden": 2, "global_golden": 1, "previous": 1, "none": 1}},
              "recency": {"last_marked_at": "2026-02-26T15:00:00Z", "last_actor_id": "user-2"}
            }
          ]' > /tmp/sample-insights.json

          # Create sample forecasts with signal quality metrics and SLO alerts
          echo '[
            {
              "thread_id": "t-1",
              "risk_score": 65,
              "trend": "degrading",
              "drivers": ["baseline_none_rate_high"],
              "recommended_focus": "Aumentar cobertura de golden",
              "confidence": 0.7,
              "volatility": 45,
              "calibration_notes": ["Dados histÃ³ricos moderados"],
              "slo_alerts": [
                {
                  "alert_id": "slo-latency-1",
                  "severity": "warning",
                  "message": "LatÃªncia prÃ³xima ao limite SLO",
                  "threshold": 0.95,
                  "current_value": 0.92
                }
              ],
              "playbook_executions": [
                {
                  "playbook_id": "pb-auto-scale",
                  "status": "completed",
                  "executed_at": "2026-02-27T06:00:00Z",
                  "actions_taken": ["scale_up", "notify_team"]
                }
              ],
              "executed_actions": ["scale_up", "notify_team"],
              "pending_actions": ["review_capacity"]
            },
            {
              "thread_id": "t-2",
              "risk_score": 35,
              "trend": "improving",
              "drivers": [],
              "recommended_focus": "Manter prÃ¡ticas atuais",
              "confidence": 0.8,
              "volatility": 25,
              "calibration_notes": ["Alta confianÃ§a"],
              "slo_alerts": [],
              "playbook_executions": [],
              "executed_actions": ["validate_setup"],
              "pending_actions": []
            }
          ]' > /tmp/sample-forecasts.json
          
          # Create sample recommendations
          echo '[
            {
              "thread_id": "t-1",
              "recommendations": [
                {"action_id": "create_golden", "severity": "warning", "suppressed": false, "suppression_reason": ""}
              ]
            },
            {
              "thread_id": "t-2",
              "recommendations": [
                {"action_id": "review", "severity": "info", "suppressed": true, "suppression_reason": "Cooldown ativo"}
              ]
            }
          ]' > /tmp/sample-recommendations.json

          # Generate report with recommendations
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file /tmp/sample-insights.json \
            --forecasts-file /tmp/sample-forecasts.json \
            --recommendations-file /tmp/sample-recommendations.json \
            --output /tmp/test-report.md

          # Verify output
          cat /tmp/test-report.md
          
          # Check for expected content
          grep -q "Total Threads" /tmp/test-report.md || (echo "Missing Total Threads" && exit 1)
          grep -q "Total Golden Marked" /tmp/test-report.md || (echo "Missing Total Golden Marked" && exit 1)
          grep -q "Forecast Summary" /tmp/test-report.md || (echo "Missing Forecast Summary" && exit 1)
          grep -q "Top 3 Threads by Risk" /tmp/test-report.md || (echo "Missing Top 3 Threads" && exit 1)
          grep -q "Signal Quality" /tmp/test-report.md || (echo "Missing Signal Quality" && exit 1)
          grep -q "Avg Confidence" /tmp/test-report.md || (echo "Missing Avg Confidence" && exit 1)
          
          # Check for Task D requirements
          grep -q "SLO Alerts Evidence" /tmp/test-report.md || (echo "Missing SLO Alerts Evidence" && exit 1)
          grep -q "Playbook Execution Status" /tmp/test-report.md || (echo "Missing Playbook Execution Status" && exit 1)
          grep -q "AÃ§Ãµes Executadas e Pendentes" /tmp/test-report.md || (echo "Missing AÃ§Ãµes Executadas e Pendentes" && exit 1)
          
          echo "âœ… Report generator test passed"

      - name: Test report generator JSON output
        run: |
          echo '[{"thread_id": "t-1", "totals": {"marked_total": 1}, "policy": {"denied_total": 0}, "baseline": {"by_source": {"none": 1}}}]' > /tmp/minimal.json
          echo '[{"thread_id": "t-1", "risk_score": 50, "trend": "stable", "drivers": [], "recommended_focus": "Test"}]' > /tmp/minimal-forecasts.json
          
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file /tmp/minimal.json \
            --forecasts-file /tmp/minimal-forecasts.json \
            --format json \
            --output /tmp/test-report.json
          
          # Verify JSON structure
          python3 -c "
          import json
          with open('/tmp/test-report.json') as f:
              data = json.load(f)
              assert 'generated_at' in data
              assert 'metrics' in data
              assert 'forecasts' in data
              assert 'top_risks' in data
              assert 'has_staging_url' in data
              print('JSON structure valid')
          "
          
          echo "âœ… JSON output test passed"

      - name: Test forecast delta calculation
        run: |
          # Create current forecasts
          echo '[
            {"thread_id": "t-1", "risk_score": 75, "trend": "degrading", "drivers": [], "recommended_focus": "Test"},
            {"thread_id": "t-2", "risk_score": 30, "trend": "improving", "drivers": [], "recommended_focus": "Test"}
          ]' > /tmp/current-forecasts.json
          
          # Create a previous report with risk scores
          cat > /tmp/previous-report.md << 'EOF'
          # Editorial Operations Report
          
          ## Forecast Summary
          
          | Thread | Risk Score | Delta | Trend | Focus |
          |--------|-----------|-------|-------|-------|
          | t-1 | 65 | â€” | stable | Test |
          | t-2 | 35 | â€” | stable | Test |
          EOF
          
          # Generate report with delta
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file /tmp/sample-insights.json \
            --forecasts-file /tmp/current-forecasts.json \
            --previous-report /tmp/previous-report.md \
            --output /tmp/delta-report.md
          
          # Verify delta calculation
          cat /tmp/delta-report.md
          
          grep -q "ðŸ“ˆ" /tmp/delta-report.md || (echo "Missing increase indicator" && exit 1)
          grep -q "ðŸ“‰" /tmp/delta-report.md || (echo "Missing decrease indicator" && exit 1)
          
          echo "âœ… Delta calculation test passed"

      - name: Test SKIPPED behavior without staging URL
        run: |
          # Create sample data
          echo '[]' > /tmp/empty-insights.json
          echo '[]' > /tmp/empty-forecasts.json
          
          # Generate report without staging URL
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file /tmp/empty-insights.json \
            --forecasts-file /tmp/empty-forecasts.json \
            --output /tmp/skipped-report.md
          
          # Verify SKIPPED status is in report
          cat /tmp/skipped-report.md
          
          grep -q "SKIPPED" /tmp/skipped-report.md || (echo "Missing SKIPPED status" && exit 1)
          grep -q "No staging URL" /tmp/skipped-report.md || (echo "Missing staging URL message" && exit 1)
          
          echo "âœ… SKIPPED behavior test passed"

      - name: Test structural PASS without real data
        run: |
          # Create sample/demo data
          echo '[{
            "thread_id": "sample",
            "totals": {"marked_total": 0, "by_scope": {"global": 0, "objective": 0}},
            "policy": {"denied_total": 0},
            "baseline": {"resolved_total": 0, "by_source": {}}
          }]' > /tmp/sample-only.json
          
          echo '[{
            "thread_id": "sample",
            "risk_score": 45,
            "trend": "stable",
            "recommended_focus": "Demo"
          }]' > /tmp/sample-forecast-only.json
          
          # Generate report with staging URL but sample data
          python3 09-tools/scripts/editorial_ops_report.py \
            --insights-file /tmp/sample-only.json \
            --forecasts-file /tmp/sample-forecast-only.json \
            --staging-url "https://example-staging.com" \
            --output /tmp/structural-pass-report.md
          
          # Verify report is generated
          cat /tmp/structural-pass-report.md
          
          grep -q "Editorial Operations Report" /tmp/structural-pass-report.md || (echo "Missing report header" && exit 1)
          
          echo "âœ… Structural PASS test passed"
